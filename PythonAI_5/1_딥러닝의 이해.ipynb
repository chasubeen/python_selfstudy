{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_딥러닝의 이해.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOyNMeZdvs+ehE2NjZDQL1V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **1. 딥러닝의 진화**"],"metadata":{"id":"nsvPFTHYxKjJ"}},{"cell_type":"markdown","source":["### **2012**\n","* CNN중 Alex net이라는 네트워크 구조\n","* Alex net이 1등을 차지 -> CNN이 유명해지게 됨"],"metadata":{"id":"mu3lm41txrH_"}},{"cell_type":"markdown","source":["### **2013**\n","* 아타리 게임 -> 학습을 많이 진행 -> 구석을 파서 공을 위로 올리는 방법을 터득함\n","* 딥마인드 -> 구글에 인수 -> 알파고 "],"metadata":{"id":"SxLIzymO0Ifn"}},{"cell_type":"markdown","source":["### **2014**\n","* RNN이라는 네트워크를 사용하여 중국어 -> 영어로 번역\n","* 성능의 한계가 생김\n","* attention 모델의 출현으로 성능이 급격히 좋아짐\n"],"metadata":{"id":"Lt20Ovs21Gyb"}},{"cell_type":"markdown","source":["# **2015**\n","- GANs -> 오바마 합성 사진(딥페이크)\n","- Residual Networks(ResNet): CNN의 종류 중 하나\n","- 사람이 이미지를 찾는 테스트 -> 5%의 오차율\n","- Res의 경우 3% 이내의 오차율\n"],"metadata":{"id":"Gtpn1zhw1vRl"}},{"cell_type":"markdown","source":["### **2016**\n","- 알파고 : 이세돌 9단이 컴퓨터를 이긴 마지막 인류가 됨\n"],"metadata":{"id":"Et8lH_o93qqj"}},{"cell_type":"markdown","source":["### **2017**\n","- Transformer: 번역모델\n","- RNN의 단점(= 병렬 처리의 불가능성)을 극복\n","- attention 만으로 만든 모델\n","- 자연어를 정복\n","- 딥러닝에서 가장 중요한 모델 중 하나"],"metadata":{"id":"XDqOFLuN4DcD"}},{"cell_type":"markdown","source":["### **2018**\n","- BERT\n","- Transformer에서 인코더만 따온 모델\n","- 자연어를 정답 없이 사용하게 된다.(인터넷 문장들의 데이터를 삽입)\n","- 일부 글자를 가리고, 가린 부분을 맞출 수 있도록 학습\n","- 모델을 크게 만들고 엄청난 데이터를 사용함\n","- 적은 데이터(정답)를 가지고 다시 학습시킴"],"metadata":{"id":"kYAlfCwB5DfB"}},{"cell_type":"markdown","source":["### **2019/2020**\n","* GPU 수천 대를 사용하여 큰 모델을 만듦\n","- 정답이 필요없이 실제 데이터만으로 셀프 슈퍼바이즈 러닝(-> 비지도학습)\n"],"metadata":{"id":"ZAmVQo0T84_x"}},{"cell_type":"markdown","source":["# **2. Perceptron**"],"metadata":{"id":"gx3LxSvj9pgY"}},{"cell_type":"markdown","source":["### **2-1. neuron**\n","![이미지](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bc/Neuron_Hand-tuned.svg/400px-Neuron_Hand-tuned.svg.png)\n","\n","* 수상돌기(가지돌기): 다른 뉴런들로부터 신호를 받아 특정 조건을 만족하면 축삭돌기로 보냄\n","* 축삭돌기: 전달받은 신호를 다른 뉴런으로 전달"],"metadata":{"id":"jxHuvfIJ9yza"}},{"cell_type":"markdown","source":["![이미지](https://www.thedailypost.kr/news/photo/201904/70084_60318_3157.png)  \n","**파블로프의 개 실험**\n","* 먹이를 봤을 때 반응하는 뉴런 -> 시냅스(강함) -> 침샘을 분비하는 뉴런\n","* 종을 쳤을 때 반응하는 뉴런 -> 시냅스(약함) -> 침샘분비뉴런\n","\n","> 이것을 모방해서 만든 인공신경망이 Perceptron이다.\n","\n"],"metadata":{"id":"-1aCJ5E7AQ62"}},{"cell_type":"markdown","source":["# **3. 딥러닝의 키포인트**"],"metadata":{"id":"ozPjX9fiB7Jo"}},{"cell_type":"markdown","source":["* 데이터 \n","* 모델(CNN,RNN,트랜스포머, multi-layer perceptron, ...)\n","* 알고리즘(GradientDescent를 기초로 많은 알고리즘이 만들어짐)\n","* loss fuction"],"metadata":{"id":"SWszLLX3CIkM"}},{"cell_type":"markdown","source":["### **3-1. Neural Network에서의 핵심**\n","* Deep Neural Network: layer 수가 최소 2개 이상인 network  \n","cf> 원래 딥러닝의 경우 3개 이상의 layer를 요구함  but 2개의 layer을 합성하여 새로운 layer를 만들 수도 있음 -> 최소 2개 이상의 layer만 있으면 된다.\n","* Loss Fuction(Cost Function): Neural Network이 얼마나 성능이 좋은지 또는 나쁜지에 대한 척도\n","* loss 값을 내부적으로 줄이는 방법: 미분(접선의 기울기) 이용\n","> 적절한 weight를 찾는 것이 핵심\n"],"metadata":{"id":"SMo6mIXYDM0D"}}]}