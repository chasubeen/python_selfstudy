{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. 라이브러리 import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 모델 가중치 복원\n",
    "\n",
    "PATH_TO_MODEL = \"./convnet.pth\"\n",
    "model.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. 이미지 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 로딩\n",
    "image = Image.open(\"./digit_image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image):\n",
    "    gray_image = transforms.functional.to_grayscale(image)\n",
    "    resized_image = transforms.functional.resize(gray_image, (28, 28))\n",
    "    input_image_tensor = transforms.functional.to_tensor(resized_image)\n",
    "    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, \n",
    "                                                              (0.1302,), (0.3069,))\n",
    "    return input_image_tensor_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 정의\n",
    "\n",
    "input_tensor = image_to_tensor(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. 모델 추적**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_input = torch.ones(1, 1, 28, 28)\n",
    "traced_model = torch.jit.trace(model, demo_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.ConvNet,\n",
       "      %x.1 : Float(1, 1, 28, 28, strides=[784, 784, 28, 1], requires_grad=0, device=cpu)):\n",
       "  %fc2 : __torch__.torch.nn.modules.linear.___torch_mangle_2.Linear = prim::GetAttr[name=\"fc2\"](%self.1)\n",
       "  %dp2 : __torch__.torch.nn.modules.dropout.___torch_mangle_1.Dropout2d = prim::GetAttr[name=\"dp2\"](%self.1)\n",
       "  %fc1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"fc1\"](%self.1)\n",
       "  %dp1 : __torch__.torch.nn.modules.dropout.Dropout2d = prim::GetAttr[name=\"dp1\"](%self.1)\n",
       "  %cn2 : __torch__.torch.nn.modules.conv.___torch_mangle_0.Conv2d = prim::GetAttr[name=\"cn2\"](%self.1)\n",
       "  %cn1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name=\"cn1\"](%self.1)\n",
       "  %123 : Tensor = prim::CallMethod[name=\"forward\"](%cn1, %x.1)\n",
       "  %input.3 : Float(1, 16, 26, 26, strides=[10816, 676, 26, 1], requires_grad=0, device=cpu) = aten::relu(%123) # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:1299:0\n",
       "  %124 : Tensor = prim::CallMethod[name=\"forward\"](%cn2, %input.3)\n",
       "  %input.7 : Float(1, 32, 24, 24, strides=[18432, 576, 24, 1], requires_grad=0, device=cpu) = aten::relu(%124) # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:1299:0\n",
       "  %70 : int = prim::Constant[value=2]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %71 : int = prim::Constant[value=2]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %72 : int[] = prim::ListConstruct(%70, %71)\n",
       "  %73 : int[] = prim::ListConstruct()\n",
       "  %74 : int = prim::Constant[value=0]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %75 : int = prim::Constant[value=0]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %76 : int[] = prim::ListConstruct(%74, %75)\n",
       "  %77 : int = prim::Constant[value=1]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %78 : int = prim::Constant[value=1]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %79 : int[] = prim::ListConstruct(%77, %78)\n",
       "  %80 : bool = prim::Constant[value=0]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %input.9 : Float(1, 32, 12, 12, strides=[4608, 144, 12, 1], requires_grad=0, device=cpu) = aten::max_pool2d(%input.7, %72, %73, %76, %79, %80) # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:719:0\n",
       "  %125 : Tensor = prim::CallMethod[name=\"forward\"](%dp1, %input.9)\n",
       "  %85 : int = prim::Constant[value=1]() # C:\\Users\\doroc\\AppData\\Local\\Temp\\ipykernel_19028\\2548233835.py:20:0\n",
       "  %86 : int = prim::Constant[value=-1]() # C:\\Users\\doroc\\AppData\\Local\\Temp\\ipykernel_19028\\2548233835.py:20:0\n",
       "  %input.11 : Float(1, 4608, strides=[4608, 1], requires_grad=0, device=cpu) = aten::flatten(%125, %85, %86) # C:\\Users\\doroc\\AppData\\Local\\Temp\\ipykernel_19028\\2548233835.py:20:0\n",
       "  %126 : Tensor = prim::CallMethod[name=\"forward\"](%fc1, %input.11)\n",
       "  %input.15 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::relu(%126) # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:1299:0\n",
       "  %127 : Tensor = prim::CallMethod[name=\"forward\"](%dp2, %input.15)\n",
       "  %128 : Tensor = prim::CallMethod[name=\"forward\"](%fc2, %127)\n",
       "  %94 : int = prim::Constant[value=1]() # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:1769:0\n",
       "  %95 : NoneType = prim::Constant()\n",
       "  %96 : Float(1, 10, strides=[10, 1], requires_grad=0, device=cpu) = aten::log_softmax(%128, %94, %95) # c:\\Users\\doroc\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\functional.py:1769:0\n",
       "  return (%96)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  fc2 = self.fc2\n",
      "  dp2 = self.dp2\n",
      "  fc1 = self.fc1\n",
      "  dp1 = self.dp1\n",
      "  cn2 = self.cn2\n",
      "  cn1 = self.cn1\n",
      "  input = torch.relu((cn1).forward(x, ))\n",
      "  input0 = torch.relu((cn2).forward(input, ))\n",
      "  input1 = torch.max_pool2d(input0, [2, 2], annotate(List[int], []), [0, 0], [1, 1])\n",
      "  input2 = torch.flatten((dp1).forward(input1, ), 1)\n",
      "  input3 = torch.relu((fc1).forward(input2, ))\n",
      "  _0 = (fc2).forward((dp2).forward(input3, ), )\n",
      "  return torch.log_softmax(_0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 추적한 모델 뒤에 있는 정확한 토치스크립트 코드 살펴보기\n",
    "\n",
    "print(traced_model.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추적 파일 내보내기(저장하기)\n",
    "\n",
    "torch.jit.save(traced_model, 'traced_convnet.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. 모델 로딩하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_traced_model = torch.jit.load('traced_convnet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.3505e+00, -1.2089e+01, -2.2391e-03, -8.9248e+00, -9.8197e+00,\n",
       "         -1.3350e+01, -9.0460e+00, -1.4492e+01, -6.3023e+00, -1.2283e+01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-9.3505e+00, -1.2089e+01, -2.2391e-03, -8.9248e+00, -9.8197e+00,\n",
       "         -1.3350e+01, -9.0460e+00, -1.4492e+01, -6.3023e+00, -1.2283e+01]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_traced_model(input_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 추적한 모델이 원래의 모델과 동일하게 제대로 작동하고 있음을 확인할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
