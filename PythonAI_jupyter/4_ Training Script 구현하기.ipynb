{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6d4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e89fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequential_model(input_shape):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            # input\n",
    "            layers.Input(input_shape),\n",
    "            \n",
    "            # 1번쨰 layer\n",
    "            layers.Conv2D(64,3,strides = 1,activation = \"relu\",padding = 'same'), # 이미지를 위한 레이어\n",
    "            layers.Conv2D(64,3,strides = 1,activation = \"relu\",padding = 'same'),\n",
    "            layers.MaxPool2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            \n",
    "            # 2번쨰 layer\n",
    "            layers.Conv2D(128,3,strides = 1,activation = \"relu\",padding = 'same'),\n",
    "            layers.Conv2D(128,3,strides = 1,activation = \"relu\",padding = 'same'),\n",
    "            layers.MaxPool2D(),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            \n",
    "            # Classifier\n",
    "            layers.GlobalMaxPool2D(),\n",
    "            layers.Dense(128,activation = \"relu\"),\n",
    "            layers.Dense(1,activation = \"sigmoid\")\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbb0204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 256, 256, 64)      1792      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 256, 256, 64)      36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 128, 128, 64)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128, 128, 64)     256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 128, 64)      0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 128, 128, 128)     147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 64, 64, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64, 64, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64, 64, 128)       0         \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 128)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 277,569\n",
      "Trainable params: 277,185\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (256,256,3)\n",
    "model = get_sequential_model(input_shape)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = 'accuracy'\n",
    ")\n",
    "\n",
    "model.summary() # 모델의 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0d2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, csv_path, fold, image_size, mode='train', shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.df = self.df[self.df['fold'] != self.fold]\n",
    "        elif self.mode == 'val':\n",
    "            self.df = self.df[self.df['fold'] == self.fold]\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        #### https://github.com/tensorflow/models/issues/3134\n",
    "        #### 파일 이슈 -> 삭제\n",
    "        invalid_filenames = [\n",
    "            'Egyptian_Mau_14',\n",
    "            'Egyptian_Mau_139',\n",
    "            'Egyptian_Mau_145',\n",
    "            'Egyptian_Mau_156',\n",
    "            'Egyptian_Mau_167',\n",
    "            'Egyptian_Mau_177',\n",
    "            'Egyptian_Mau_186',\n",
    "            'Egyptian_Mau_191',\n",
    "            'Abyssinian_5',\n",
    "            'Abyssinian_34',\n",
    "            'chihuahua_121',\n",
    "            'beagle_116'\n",
    "        ]\n",
    "        self.df = self.df[~self.df['file_name'].isin(invalid_filenames)]\n",
    "        \n",
    "        self.on_epoch_end()\n",
    "\n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # len()\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.df) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        strt = idx * self.batch_size\n",
    "        fin = (idx + 1) * self.batch_size\n",
    "        data = self.df.iloc[strt:fin]\n",
    "        batch_x, batch_y = self.get_data(data)\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "        \n",
    "    def get_data(self, data):\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for _, r in data.iterrows():\n",
    "            file_name = r['file_name']\n",
    "            image = cv2.imread(f'data/images/{file_name}.jpg')\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "            image = image / 255.\n",
    "        \n",
    "            label = int(r['species']) - 1\n",
    "            \n",
    "            batch_x.append(image)\n",
    "            batch_y.append(label)\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4636a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'data/kfolds.csv'\n",
    "\n",
    "train_generator = DataGenerator(\n",
    "    batch_size = 128,\n",
    "    csv_path = csv_path,\n",
    "    fold = 1, \n",
    "    image_size = 256,\n",
    "    mode = 'train',\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "valid_generator = DataGenerator(\n",
    "    batch_size = 128,\n",
    "    csv_path = csv_path,\n",
    "    fold = 1, \n",
    "    image_size = 256,\n",
    "    mode = 'val',\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa46b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping\n",
    "# 무조건 epoch을 많이 돌린 후, 특정 시점에 멈추게 하는 기능\n",
    "# monitor : EarlyStopping의 기준이 되는 값\n",
    "# val_loss는 val_loss 값이 더이상 감소되지 않을 경우 EarlyStopping을 적용\n",
    "# patience: Training이 진행됨에도 더이상 monitor되는 값의 개선이 없을 경우, 몇 번의 epoch을 진행할 지 정하는 값\n",
    "# mode: monitor되는 값이 최소가 되야 하는지, 최대가 되어야 하는지 설정하는 값\n",
    "# restore_best_weights: true라면 training이 끝난 후, model의 weight를 monitor하고 있던 값이 가장 좋았을 경우 weight로 복원,\n",
    "# False라면 마지막 training이 끝난 후 weight로 설정\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',patience = 3,verbose = 1,mode = 'min', restore_best_weights = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45b3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReduceLROnPlateau\n",
    "# monitor: ReduceLROnPlateau의 기준이 되는 값, val_loss가 더이상 감소되지 않을 경우 ReduceLROnPlateau를 적용\n",
    "# factor: learning rate를 얼마나 감소시킬지 정하는 값, 새로운 learning rate = learning rate * factor\n",
    "# patience : Training이 진행됨에도 더이상 monitor되는 값의 개선이 없을 경우, 최적의 monitor 값을 기준으로\n",
    "# learning rate를 조절할지의 값을 설정함\n",
    "\n",
    "reduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', factor = 0.1, patience = 10, verbose = 1, model = 'min', min_lr = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fa4f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 경로 \n",
    "# 모델 경로를 '{epoch:02d}-{val_loss:.2f}.hdf5'라고 하면, 앞에 명시한 문자열로 파일을 저장함\n",
    "# 예: 01-0.12.h5\n",
    "# save_weights_only: True -> weights만 저장, False -> 모델, layer, weights 모두 저장\n",
    "filepath = '{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor = 'val_loss', verbose=1, save_best_only=True,\n",
    "    save_weights_only=False, mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7717ac0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.9414 - accuracy: 0.6433 \n",
      "Epoch 1: val_loss improved from inf to 0.65266, saving model to 01-0.65.hdf5\n",
      "46/46 [==============================] - 973s 21s/step - loss: 0.9414 - accuracy: 0.6433 - val_loss: 0.6527 - val_accuracy: 0.6767 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6309 - accuracy: 0.6699 \n",
      "Epoch 2: val_loss did not improve from 0.65266\n",
      "46/46 [==============================] - 970s 21s/step - loss: 0.6309 - accuracy: 0.6699 - val_loss: 0.6854 - val_accuracy: 0.5950 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.6108 - accuracy: 0.6730 \n",
      "Epoch 3: val_loss did not improve from 0.65266\n",
      "46/46 [==============================] - 971s 21s/step - loss: 0.6108 - accuracy: 0.6730 - val_loss: 0.6597 - val_accuracy: 0.6481 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5990 - accuracy: 0.6837 \n",
      "Epoch 4: val_loss improved from 0.65266 to 0.64792, saving model to 04-0.65.hdf5\n",
      "46/46 [==============================] - 974s 21s/step - loss: 0.5990 - accuracy: 0.6837 - val_loss: 0.6479 - val_accuracy: 0.6814 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5881 - accuracy: 0.6971 \n",
      "Epoch 5: val_loss improved from 0.64792 to 0.62493, saving model to 05-0.62.hdf5\n",
      "46/46 [==============================] - 974s 21s/step - loss: 0.5881 - accuracy: 0.6971 - val_loss: 0.6249 - val_accuracy: 0.6841 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.6898 \n",
      "Epoch 6: val_loss did not improve from 0.62493\n",
      "46/46 [==============================] - 976s 21s/step - loss: 0.5905 - accuracy: 0.6898 - val_loss: 0.6280 - val_accuracy: 0.6835 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5719 - accuracy: 0.7029 \n",
      "Epoch 7: val_loss did not improve from 0.62493\n",
      "46/46 [==============================] - 972s 21s/step - loss: 0.5719 - accuracy: 0.7029 - val_loss: 0.6402 - val_accuracy: 0.6855 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5732 - accuracy: 0.7011 \n",
      "Epoch 8: val_loss improved from 0.62493 to 0.61390, saving model to 08-0.61.hdf5\n",
      "46/46 [==============================] - 976s 21s/step - loss: 0.5732 - accuracy: 0.7011 - val_loss: 0.6139 - val_accuracy: 0.6882 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.7106 \n",
      "Epoch 9: val_loss improved from 0.61390 to 0.60512, saving model to 09-0.61.hdf5\n",
      "46/46 [==============================] - 974s 21s/step - loss: 0.5582 - accuracy: 0.7106 - val_loss: 0.6051 - val_accuracy: 0.7039 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.7257 \n",
      "Epoch 10: val_loss did not improve from 0.60512\n",
      "46/46 [==============================] - 976s 21s/step - loss: 0.5461 - accuracy: 0.7257 - val_loss: 0.6524 - val_accuracy: 0.6474 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data = valid_generator,\n",
    "    epochs = 10,\n",
    "    callbacks = [\n",
    "        early_stopping,\n",
    "        reduce_on_plateau,\n",
    "        model_checkpoint\n",
    "        \n",
    "    ],\n",
    "    verbose = 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
